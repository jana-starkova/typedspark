{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType\n",
    "\n",
    "from typedspark._core.column import Column\n",
    "from typedspark import Schema\n",
    "\n",
    "\n",
    "class A(Schema):\n",
    "    a: Column[LongType]\n",
    "    b: Column[StringType]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/06/30 12:32:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/30 12:32:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.Builder().config(\"spark.ui.showConsoleProgress\", \"false\").getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded in comparison",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/IPython/core/formatters.py:706\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    699\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    700\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    701\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    702\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    703\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    704\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    705\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 706\u001b[0m printer\u001b[39m.\u001b[39;49mpretty(obj)\n\u001b[1;32m    707\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    708\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m callable(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39;49m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39;49m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:1052\u001b[0m, in \u001b[0;36mColumn.__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1051\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__repr__\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m-> 1052\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mColumn<\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m>\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jc\u001b[39m.\u001b[39mtoString()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:549\u001b[0m, in \u001b[0;36mColumn.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(item)\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[item]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:557\u001b[0m, in \u001b[0;36mColumn.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubstr(k\u001b[39m.\u001b[39mstart, k\u001b[39m.\u001b[39mstop)\n\u001b[1;32m    556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mreturn\u001b[39;00m _bin_op(\u001b[39m\"\u001b[39;49m\u001b[39mapply\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m, k)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:163\u001b[0m, in \u001b[0;36m_bin_op.<locals>._\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     other: Union[\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLiteralType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDecimalLiteral\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDateTimeLiteral\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    162\u001b[0m     jc \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_jc \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Column) \u001b[39melse\u001b[39;00m other\n\u001b[0;32m--> 163\u001b[0m     njc \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jc, name)(jc)\n\u001b[1;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(njc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:549\u001b[0m, in \u001b[0;36mColumn.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(item)\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[item]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:557\u001b[0m, in \u001b[0;36mColumn.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubstr(k\u001b[39m.\u001b[39mstart, k\u001b[39m.\u001b[39mstop)\n\u001b[1;32m    556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mreturn\u001b[39;00m _bin_op(\u001b[39m\"\u001b[39;49m\u001b[39mapply\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39;49m, k)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:163\u001b[0m, in \u001b[0;36m_bin_op.<locals>._\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     other: Union[\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLiteralType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDecimalLiteral\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDateTimeLiteral\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    162\u001b[0m     jc \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_jc \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Column) \u001b[39melse\u001b[39;00m other\n\u001b[0;32m--> 163\u001b[0m     njc \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jc, name)(jc)\n\u001b[1;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(njc)\n",
      "    \u001b[0;31m[... skipping similar frames: Column.__getattr__ at line 549 (983 times), Column.__getitem__ at line 557 (983 times), _bin_op.<locals>._ at line 163 (982 times)]\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:163\u001b[0m, in \u001b[0;36m_bin_op.<locals>._\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(\n\u001b[1;32m    159\u001b[0m     \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    160\u001b[0m     other: Union[\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLiteralType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDecimalLiteral\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDateTimeLiteral\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m    161\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    162\u001b[0m     jc \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_jc \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Column) \u001b[39melse\u001b[39;00m other\n\u001b[0;32m--> 163\u001b[0m     njc \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jc, name)(jc)\n\u001b[1;32m    164\u001b[0m     \u001b[39mreturn\u001b[39;00m Column(njc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:549\u001b[0m, in \u001b[0;36mColumn.__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[39mif\u001b[39;00m item\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m__\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    548\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(item)\n\u001b[0;32m--> 549\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[item]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:557\u001b[0m, in \u001b[0;36mColumn.__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msubstr(k\u001b[39m.\u001b[39mstart, k\u001b[39m.\u001b[39mstop)\n\u001b[1;32m    556\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 557\u001b[0m     \u001b[39mreturn\u001b[39;00m _bin_op(\u001b[39m\"\u001b[39;49m\u001b[39mapply\u001b[39;49m\u001b[39m\"\u001b[39;49m)(\u001b[39mself\u001b[39m, k)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/column.py:160\u001b[0m, in \u001b[0;36m_bin_op\u001b[0;34m(name, doc)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_bin_op\u001b[39m(\n\u001b[1;32m    151\u001b[0m     name: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    152\u001b[0m     doc: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary operator\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    153\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Callable[\n\u001b[1;32m    154\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, Union[\u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mLiteralType\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDecimalLiteral\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mDateTimeLiteral\u001b[39m\u001b[39m\"\u001b[39m]], \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m ]:\n\u001b[1;32m    156\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Create a method for given binary operator\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m_\u001b[39m(\n\u001b[1;32m    159\u001b[0m         \u001b[39mself\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m--> 160\u001b[0m         other: Union[\u001b[39m\"\u001b[39;49m\u001b[39mColumn\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mLiteralType\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDecimalLiteral\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mDateTimeLiteral\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    161\u001b[0m     ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mColumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    162\u001b[0m         jc \u001b[39m=\u001b[39m other\u001b[39m.\u001b[39m_jc \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(other, Column) \u001b[39melse\u001b[39;00m other\n\u001b[1;32m    163\u001b[0m         njc \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jc, name)(jc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/typedspark/lib/python3.9/typing.py:274\u001b[0m, in \u001b[0;36m_tp_cache.<locals>.decorator.<locals>.inner\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    272\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[1;32m    273\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[39mreturn\u001b[39;00m cached(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    275\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    276\u001b[0m         \u001b[39mpass\u001b[39;00m  \u001b[39m# All real errors (not unhashable args) are raised below.\u001b[39;00m\n",
      "\u001b[0;31mRecursionError\u001b[0m: maximum recursion depth exceeded in comparison"
     ]
    }
   ],
   "source": [
    "A.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: string]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typedspark._utils.create_dataset import create_empty_dataset\n",
    "\n",
    "\n",
    "df_a = create_empty_dataset(spark, A)\n",
    "df_b = create_empty_dataset(spark, A)\n",
    "\n",
    "df_a.join(df_b, A.a.str)\n",
    "df_a.unionByName(df_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nanneaben/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/nanneaben/opt/anaconda3/envs/typedspark/lib/python3.9/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "(\n",
    "    spark.createDataFrame(\n",
    "        pd.DataFrame(\n",
    "            dict(\n",
    "                name=[\"Jack\", \"John\", \"Jane\"],\n",
    "                age=[20, 30, 40],\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    .createOrReplaceTempView(\"person_table\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import load_table\n",
    "\n",
    "df, Person = load_table(spark, \"person_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StringType"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Person.name.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typedspark import Schema, Column, StructType, create_partially_filled_dataset\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "class Values(Schema):\n",
    "    a: Column[IntegerType]\n",
    "    b: Column[IntegerType]\n",
    "\n",
    "class Container(Schema):\n",
    "    values: Column[StructType[Values]]\n",
    "\n",
    "create_partially_filled_dataset(\n",
    "    spark,\n",
    "    Container,\n",
    "    {\n",
    "        Container.values: create_partially_filled_dataset(\n",
    "            spark,\n",
    "            Values,\n",
    "            {Values.a: [1, 2, 3]},\n",
    "        ).collect(),\n",
    "    }\n",
    ").createOrReplaceTempView(\"structtype_table\")\n",
    "\n",
    "container, ContainerSchema = load_table(spark, \"structtype_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "typedspark._core.datatypes.StructType[typedspark._schema.schema.DynamicallyLoadedSchema]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ContainerSchema.values.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(a = b)'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Values.a == Values.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typedspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
